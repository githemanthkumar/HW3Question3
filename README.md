# HW3Question3
This Code:

Loads a small text sample and tokenizes characters.

Converts text into sequences of character indices for training.

Defines an LSTM-based RNN model to predict the next character.

Trains the model using categorical cross-entropy loss.

Generates text by sampling characters based on temperature scaling.

Lower temperature (e.g., 0.5) produces more predictable text.

Higher temperature (e.g., 1.5) increases randomness and creativity.
